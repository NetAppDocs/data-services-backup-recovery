---
sidebar: sidebar
permalink: prev-ontap-protect-overview.html
keywords: backing up, back up, backup, backup application data, Amazon Web services, AWS, NetApp Backup and Recovery, Oracle database, Microsoft SQL Server database, SAP HANA database
summary: Protect your VMware workloads with NetApp NetApp Backup and Recovery. 
---

= Protect your ONTAP volume data using NetApp Backup and Recovery
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
NetApp Backup and Recovery provides backup and restore capabilities for protection and long-term archive of your ONTAP volume data. You can implement a 3-2-1 strategy where you have 3 copies of your source data on 2 different storage systems along with 1 copy in the cloud.

NOTE: To switch to and from NetApp Backup and Recovery workloads, refer to link:br-start-switch-ui.html[Switch to different NetApp Backup and Recovery workloads].


After activation, backup and recovery creates block-level, incremental forever backups that are stored on another ONTAP cluster and in object storage in the cloud. In addition to your source volume, you'll have a:

* Snapshot of the volume on the source system
* Replicated volume on a different storage system
* Backup of the volume in object storage 

image:diagram-321-overview-unified.png[A diagram showing how backup files exist on the source system as snapshots, as replicated volumes on the secondary storage system, and as a backup files in object storage.]

NetApp Backup and Recovery leverages NetApp's SnapMirror data replication technology to ensure that all the backups are fully synchronized by creating snapshots and transferring them to the backup locations.

The benefits of the 3-2-1 approach include:

* Multiple data copies protect against internal and external cybersecurity threats.
* Using different types of media helps you recover if one type fails.
* You can quickly restore from the onsite copy, and use the offsite copies if the onsite copy is compromised.

When necessary, you can restore an entire _volume_, a _folder_, or one or more _files_, from any of the backup copies to the same or different system.

== Features

*Replication features:*

* Replicate data between ONTAP storage systems to support backup and disaster recovery.
* Ensure the reliability of your DR environment with high availability.
* Native ONTAP in-flight encryption set up via Pre-Shared Key (PSK) between the two systems.
* Copied data is immutable until you make it writable and ready to use.
* Replication is self-healing in the event of a transfer failure.
* When compared to https://docs.netapp.com/us-en/data-services-replication/index.html[NetApp  Replication^], the replication in NetApp Backup and Recovery includes the following features:
** Replicate multiple FlexVol volumes at a time to a secondary system.
** Restore a replicated volume to the source system or to a different system using the UI.

//** Restore files and folders

See link:br-reference-limitations.html[Replication limitations for ONTAP volumes] for a list of replication features that are unavailable with NetApp Backup and Recovery for ONTAP volumes.

*Backup-to-object features:*

* Back up independent copies of your data volumes to low-cost object storage.
* Apply a single backup policy to all volumes in a cluster, or assign different backup policies to volumes that have unique recovery point objectives.
* Create a backup policy to be applied to all future volumes created in the cluster.
* Make immutable backup files so they are locked and protected for the retention period.
* Scan backup files for possible ransomware attack - and remove/replace infected backups automatically.
* Tier older backup files to archival storage to save costs.
* Delete the backup relationship so you can archive unneeded source volumes while retaining volume backups.
* Back up from cloud to cloud, and from on-premises systems to public or private cloud.
* Backup data is secured with AES-256 bit encryption at-rest and TLS 1.2 HTTPS connections in-flight.
* Use your own customer-managed keys for data encryption instead of using the default encryption keys from your cloud provider.
* Support for up to 4,000 backups of a single volume.

*Restore features:*

* Restore data from a specific point in time from local snapshots, replicated volumes, or backed up volumes in object storage.
* Restore a volume, a folder, or individual files, to the source system or to a different system.
* Restore data to a system using a different subscription/account or that is in a different region.
* Perform a _quick restore_ of a volume from cloud storage to a Cloud Volumes ONTAP system or to an on-premises system; perfect for disaster recovery situations where you need to provide access to a volume as soon as possible.
* Restore data on a block level, placing the data directly in the location you specify, all while preserving the original ACLs.
* Browse and search file catalogs for easy selection of individual folders and files for single file restore.

== Supported systems for backup and restore operations

NetApp Backup and Recovery supports ONTAP systems and public and private cloud providers.

=== Supported regions

NetApp Backup and Recovery is supported with Cloud Volumes ONTAP in many Amazon Web Services, Microsoft Azure, and Google Cloud regions. 

https://bluexp.netapp.com/cloud-volumes-global-regions[Learn more using the Global Regions Map^]

=== Supported backup destinations

NetApp Backup and Recovery enables you to back up ONTAP volumes from the following source systems to the following secondary systems and object storage in public and private cloud providers. snapshots reside on the source system.

[cols=3*,options="header",cols="33,33,33",width="90%"]
|===

| Source system
| Secondary system (Replication)
| Destination Object Store (Backup)

//ifdef::aws[]
| Cloud Volumes ONTAP in AWS
| Cloud Volumes ONTAP in AWS
On-premises ONTAP system
| Amazon S3
endif::aws[]
//ifdef::azure[]
| Cloud Volumes ONTAP in Azure
| Cloud Volumes ONTAP in Azure
On-premises ONTAP system
| Azure Blob
endif::azure[]
ifdef::gcp[]
| Cloud Volumes ONTAP in Google
| Cloud Volumes ONTAP in Google
On-premises ONTAP system
| Google Cloud Storage
endif::gcp[]
| On-premises ONTAP system
| Cloud Volumes ONTAP
On-premises ONTAP system
|
//ifdef::aws[]
Amazon S3
endif::aws[]
//ifdef::azure[]
Azure Blob
endif::azure[]
ifdef::gcp[]
Google Cloud Storage
endif::gcp[]
NetApp StorageGRID
ONTAP S3

|===

=== Supported restore destinations

You can restore ONTAP data from a backup file that resides in a secondary system (a replicated volume) or in object storage (a backup file) to the following systems. snapshots reside on the source system and can be restored only to that same system.

[cols=3*,options="header",cols="33,33,33",width="90%"]
|===

2+^| Backup File Location
| Destination system

| *Object Store (Backup)* | *Secondary System (Replication)* |
//ifdef::aws[]
| Amazon S3 | Cloud Volumes ONTAP in AWS
On-premises ONTAP system
| Cloud Volumes ONTAP in AWS
On-premises ONTAP system
endif::aws[]
//ifdef::azure[]
| Azure Blob | Cloud Volumes ONTAP in Azure
On-premises ONTAP system
| Cloud Volumes ONTAP in Azure
On-premises ONTAP system
endif::azure[]
ifdef::gcp[]
| Google Cloud Storage | Cloud Volumes ONTAP in Google
On-premises ONTAP system
| Cloud Volumes ONTAP in Google
On-premises ONTAP system
endif::gcp[]
| NetApp StorageGRID | On-premises ONTAP system
Cloud Volumes ONTAP
| On-premises ONTAP system
| ONTAP S3 | On-premises ONTAP system
Cloud Volumes ONTAP
| On-premises ONTAP system
//Cloud Volumes ONTAP

|===

Note that references to "on-premises ONTAP systems" includes FAS, AFF, and ONTAP Select systems.

== Supported volumes

NetApp Backup and Recovery supports the following types of volumes:

* FlexVol read-write volumes
* FlexGroup volumes (requires ONTAP 9.12.1 or later) 
* SnapLock Enterprise volumes (requires ONTAP 9.11.1 or later)
* SnapLock Compliance for on-premises volumes  (requires ONTAP 9.14 or later)
* SnapMirror data protection (DP) destination volumes


NOTE: NetApp Backup and Recovery does not support backups of FlexCache volumes. 

See the sections on link:br-reference-limitations.html[Backup and restore limitations for ONTAP volumes] for additional requirements and limitations.

== Cost

There are two types of costs associated with using NetApp Backup and Recovery with ONTAP systems: resource charges and service charges. Both of these charges are for the backup to object portion of the service. 

There is no charge to create snapshots or replicated volumes - other than the disk space required to store the snapshots and replicated volumes.

*Resource charges*

Resource charges are paid to the cloud provider for object storage capacity and for writing and reading backup files to the cloud.

* For Backup to object storage, you pay your cloud provider for object storage costs.
+
Since NetApp Backup and Recovery preserves the storage efficiencies of the source volume, you pay the cloud provider object storage costs for the data _after_ ONTAP efficiencies (for the smaller amount of data after deduplication and compression have been applied).

* For restoring data using Search & Restore, certain resources are provisioned by your cloud provider, and there is per-TiB cost associated with the amount of data that is scanned by your search requests. (These resources are not needed for Browse & Restore.)
+
//ifdef::aws[]
** In AWS, https://aws.amazon.com/athena/faqs/[Amazon Athena^] and https://aws.amazon.com/glue/faqs/[AWS Glue^] resources are deployed in a new S3 bucket.
+
endif::aws[]
+
//ifdef::azure[]
** In Azure, an https://azure.microsoft.com/en-us/services/synapse-analytics/?&ef_id=EAIaIQobChMI46_bxcWZ-QIVjtiGCh2CfwCsEAAYASAAEgKwjvD_BwE:G:s&OCID=AIDcmm5edswduu_SEM_EAIaIQobChMI46_bxcWZ-QIVjtiGCh2CfwCsEAAYASAAEgKwjvD_BwE:G:s&gclid=EAIaIQobChMI46_bxcWZ-QIVjtiGCh2CfwCsEAAYASAAEgKwjvD_BwE[Azure Synapse workspace^] and https://azure.microsoft.com/en-us/services/storage/data-lake-storage/?&ef_id=EAIaIQobChMIuYz0qsaZ-QIVUDizAB1EmACvEAAYASAAEgJH5fD_BwE:G:s&OCID=AIDcmm5edswduu_SEM_EAIaIQobChMIuYz0qsaZ-QIVUDizAB1EmACvEAAYASAAEgJH5fD_BwE:G:s&gclid=EAIaIQobChMIuYz0qsaZ-QIVUDizAB1EmACvEAAYASAAEgJH5fD_BwE[Azure Data Lake Storage^] are provisioned in your storage account to store and analyze your data.
+
endif::azure[]
ifdef::gcp[]
** In Google, a new bucket is deployed, and the https://cloud.google.com/bigquery[Google Cloud BigQuery services^] are provisioned on an account/project level.
endif::gcp[]

* If you plan to restore volume data from a backup file that has been moved to archival object storage, then there's an additional per-GiB retrieval fee and per-request fee from the cloud provider.

* If you plan to scan a backup file for ransomware during the process of restoring volume data (if you have enabled DataLock and Ransomware Resilience for your cloud backups), then you'll incur extra egress costs from your cloud provider as well.

*Service charges*

Service charges are paid to NetApp and cover both the cost to _create_ backups to object storage and to _restore_ volumes, or files, from those backups. You pay only for the data that you protect in object storage, calculated by the source logical used capacity (_before_ ONTAP efficiencies) of ONTAP volumes which are backed up to object storage. This capacity is also known as Front-End Terabytes (FETB).

There are three ways to pay for the Backup service. The first option is to subscribe from your cloud provider, which enables you to pay per month. The second option is to get an annual contract. The third option is to purchase licenses directly from NetApp. 

== Licensing

NetApp Backup and Recovery is available with the following consumption models:

* *BYOL*: A license purchased from NetApp that can be used with any cloud provider.
* *PAYGO*: An hourly subscription from your cloud provider's marketplace.
* *Annual*: An annual contract from your cloud provider's marketplace.

A Backup license is required only for backup and restore from object storage. Creating snapshots and replicated volumes do not require a license.

=== Bring your own license

BYOL is term-based (1, 2, or 3 years) _and_ capacity-based in 1 TiB increments. You pay NetApp to use the service for a period of time, say 1 year, and for a maximum amount capacity, say 10 TiB.

You'll receive a serial number that you enter in the NetApp Console to enable the service. When either limit is reached, you'll need to renew the license. The Backup BYOL license applies to all source systems associated with your NetApp Console organization or account.

link:br-start-licensing.html[Learn how to manage your BYOL licenses].

=== Pay-as-you-go subscription

NetApp Backup and Recovery offers consumption-based licensing in a pay-as-you-go model. After subscribing through your cloud provider's marketplace, you pay per GiB for data that's backed up â€” there's no up-front payment. You are billed by your cloud provider through your monthly bill.

link:br-start-licensing.html[Learn how to set up a pay-as-you-go subscription].

Note that a 30-day free trial is available when you initially sign up with a PAYGO subscription.

=== Annual contract

//ifdef::aws[]
When you use AWS, two annual contracts are available for 1, 2, or 3 year terms:

* A "Cloud Backup" plan that enables you to back up Cloud Volumes ONTAP data and on-premises ONTAP data.

* A "CVO Professional" plan that enables you to bundle Cloud Volumes ONTAP and NetApp Backup and Recovery. This includes unlimited backups for Cloud Volumes ONTAP volumes charged against this license (backup capacity is not counted against the license).
endif::aws[]

//ifdef::azure[]
When you use Azure, two annual contracts are available for 1, 2, or 3 year terms:

* A "Cloud Backup" plan that enables you to back up Cloud Volumes ONTAP data and on-premises ONTAP data.

* A "CVO Professional" plan that enables you to bundle Cloud Volumes ONTAP and NetApp Backup and Recovery. This includes unlimited backups for Cloud Volumes ONTAP volumes charged against this license (backup capacity is not counted against the license).
endif::azure[]

ifdef::gcp[]
When you use GCP, you can request a private offer from NetApp, and then select the plan when you subscribe from the Google Cloud Marketplace during NetApp Backup and Recovery activation.
endif::gcp[]

link:br-start-licensing.html[Learn how to set up annual contracts].

== How NetApp Backup and Recovery works

When you enable NetApp Backup and Recovery on a Cloud Volumes ONTAP or on-premises ONTAP system, the service performs a full backup of your data. After the initial backup, all additional backups are incremental, which means that only changed blocks and new blocks are backed up. This keeps network traffic to a minimum. Backup to object storage is built on top of the https://docs.netapp.com/us-en/ontap/concepts/snapmirror-cloud-backups-object-store-concept.html[NetApp SnapMirror Cloud technology^].
//Volume snapshots are not included in the backup image. 

CAUTION: Any actions taken directly from your cloud provider environment to manage or change cloud backup files may corrupt the files and will result in an unsupported configuration.

The following image shows the relationship between each component:

image:diagram-backup-recovery-general.png["A diagram showing how NetApp Backup and Recovery communicates with the volumes on the source systems and the secondary storage system and destination object storage where the replicated volumes and backup files are located."]

This diagram shows volumes being replicated to a Cloud Volumes ONTAP system, but volumes could be replicated to an on-premises ONTAP system as well.

=== Where backups reside

Backups reside in different locations based on the type of backup:

* _Snapshots_ reside on the source volume in the source system.

* _Replicated volumes_ reside on the secondary storage system - a Cloud Volumes ONTAP or on-premises ONTAP system.

* _Backup copies_ are stored in an object store that the Console creates in your cloud account. There's one object store per cluster/system, and the Console names the object store as follows: "netapp-backup-clusteruuid". Be sure not to delete this object store.
//ifdef::aws[]
+
** In AWS, the Console enables the https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html[Amazon S3 Block Public Access feature^] on the S3 bucket.
endif::aws[]
//ifdef::azure[]
+
** In Azure, the Console uses a new or existing resource group with a storage account for the Blob container. The Console https://docs.microsoft.com/en-us/azure/storage/blobs/anonymous-read-access-prevent[blocks public access to your blob data] by default.
endif::azure[]
ifdef::gcp[]
+
** In GCP, the Console uses a new or existing project with a storage account for the Google Cloud Storage bucket.
endif::gcp[]
+
** In StorageGRID, the Console uses an existing tenant account for the S3 bucket.
+
** In ONTAP S3, the Console uses an existing user account for the S3 bucket.

If you want to change the destination object store for a cluster in the future, you'll need to link:prev-ontap-backup-manage.html[unregister NetApp Backup and Recovery for the system], and then enable NetApp Backup and Recovery using the new cloud provider information.

=== Customizable backup schedule and retention settings

When you enable NetApp Backup and Recovery for a system, all the volumes you initially select are backed up using the policies that you select. You can select separate policies for snapshots, replicated volumes, and backup files. If you want to assign different backup policies to certain volumes that have different recovery point objectives (RPO), you can create additional policies for that cluster and assign those policies to the other volumes after NetApp Backup and Recovery is activated.

You can choose a combination of hourly, daily, weekly, monthly, and yearly backups of all volumes. For backup to object you can also select one of the system-defined policies that provide backups and retention for 3 months, 1 year, and 7 years. Backup protection policies that you have created on the cluster using ONTAP System Manager or the ONTAP CLI will also appear as selections. This includes policies created using custom SnapMirror labels. 

NOTE: The Snapshot policy applied to the volume must have one of the labels that you're using in your replication policy and backup to object policy. If matching labels are not found, no backup files will be created. For example, if you want to create "weekly" replicated volumes and backup files, you must use a Snapshot policy that creates "weekly" snapshots.

//These policies are:
//
//[cols=5*,options="header",cols="35,16,16,16,26",width="80%"]
//|===
//| Backup Policy Name
//3+^| Backups per interval...
//| Max. Backups
//
//|  | *Daily* | *Weekly* | *Monthly* |
//| Netapp3MonthsRetention | 30 | 13 | 3
//| 46
//| Netapp1YearRetention | 30 | 13 | 12
//| 55
//| Netapp7YearsRetention | 30 | 53 | 84
//| 167
//
//|===

Once you reach the maximum number of backups for a category, or interval, older backups are removed so you always have the most current backups (and so obsolete backups don't continue to take up space).


TIP: The retention period for backups of data protection volumes is the same as defined in the source SnapMirror relationship. You can change this if you want by using the API.

=== Backup file protection settings

If your cluster is using ONTAP 9.11.1 or greater, you can protect your backups in object storage from deletion and ransomware attacks. Each backup policy provides a section for _DataLock and Ransomware Resilience_ that can be applied to your backup files for a specific period of time - the _retention period_. 

* _DataLock_ protects your backup files from being modified or deleted. 
* _Ransomware protection_ scans your backup files to look for evidence of a ransomware attack when a backup file is created, and when data from a backup file is being restored.

Scheduled ransomware protection scans are enabled by default. The default setting for the scan frequency is for 7 days. The scan occurs only on the latest snapshot. The scheduled scans can be disabled to reduce your costs. You can enable or disable scheduled ransomware scans on the latest snapshot by using the option on the Advanced Settings page. If you enable it, scans are performed weekly by default. You can change that schedule to days or weeks or disable it, saving costs.  

The backup retention period is the same as the backup schedule retention period, plus a maximum 31-day buffer. For example, _weekly_ backups with _5_ copies retained will lock each backup file for 5 weeks. _Monthly_ backups with _6_ copies retained will lock each backup file for 6 months.

Support is currently available when your backup destination is Amazon S3, Azure Blob, or NetApp StorageGRID. Other storage provider destinations will be added in future releases.

For more details, refer to this information: 

* link:prev-ontap-policy-object-options.html[How DataLock and Ransomware protection work].

* link:prev-ontap-policy-object-advanced-settings.html[How to update Ransomware protection options in the Advanced Settings page].

TIP: DataLock can't be enabled if you are tiering backups to archival storage.

=== Archival storage for older backup files

When using certain cloud storage you can move older backup files to a less expensive storage class/access tier after a certain number of days. You can also choose to send your backup files to archival storage immediately without being written to standard cloud storage. Note that archival storage can't be used if you have enabled DataLock.

//ifdef::aws[]
* In AWS, backups start in the _Standard_ storage class and transition to the _Standard-Infrequent Access_ storage class after 30 days.
+
If your cluster is using ONTAP 9.10.1 or greater, you can choose to tier older backups to either _S3 Glacier_ or _S3 Glacier Deep Archive_ storage in the NetApp Backup and Recovery UI after a certain number of days for further cost optimization. link:prev-reference-aws-archive-storage-tiers.html[Learn more about AWS archival storage].
endif::aws[]

//ifdef::azure[]
* In Azure, backups are associated with the _Cool_ access tier.
+
If your cluster is using ONTAP 9.10.1 or greater, you can choose to tier older backups to _Azure Archive_ storage in the NetApp Backup and Recovery UI after a certain number of days for further cost optimization. link:prev-reference-azure-archive-storage-tiers.html[Learn more about Azure archival storage].
endif::azure[]

ifdef::gcp[]
* In GCP, backups are associated with the _Standard_ storage class.
+
If your cluster is using ONTAP 9.12.1 or greater, you can choose to tier older backups to _Archive_ storage in the NetApp Backup and Recovery UI after a certain number of days for further cost optimization. link:prev-reference-gcp-archive-storage-tiers.html[Learn more about Google archival storage].
endif::gcp[]

* In StorageGRID, backups are associated with the _Standard_ storage class.
+
If your on-prem cluster is using ONTAP 9.12.1 or greater, and your StorageGRID system is using 11.4 or greater, you can archive older backup files to public cloud archival storage after a certain number of days. Current support is for AWS S3 Glacier/S3 Glacier Deep Archive or Azure Archive storage tiers. link:prev-ontap-backup-onprem-storagegrid.html[Learn more about archiving backup files from StorageGRID].

See link:prev-ontap-policy-object-options.html] for details about archiving older backup files.

== FabricPool tiering policy considerations

There are certain things you need to be aware of when the volume you are backing up resides on a FabricPool aggregate and it has an assigned tiering policy other than `none`:

* The first backup of a FabricPool-tiered volume requires reading all local and all tiered data (from the object store). A backup operation does not "reheat" the cold data tiered in object storage.
+
This operation could cause a one-time increase in cost to read the data from your cloud provider.

** Subsequent backups are incremental and do not have this effect.
** If the tiering policy is assigned to the volume when it is initially created you will not see this issue.

* Consider the impact of backups before assigning the `all` tiering policy to volumes. Because data is tiered immediately, NetApp Backup and Recovery will read data from the cloud tier rather than from the local tier. Because concurrent backup operations share the network link to the cloud object store, performance degradation might occur if network resources become saturated. In this case, you may want to proactively configure multiple network interfaces (LIFs) to decrease this type of network saturation.
